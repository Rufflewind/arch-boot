#!/usr/bin/env python3
#
# Utility for incremental transfer of large files to and from cloud storage.
# Data is divided into chunks and each chunk is encrypted before upload.
# The process can be interrupted and resumed later.
#
# Requires gpg and a storage backend command that supports:
#
#     <backend> download <remote_path> <local_path>
#     <backend> ls <remote_prefix>   # must produce a shell-quoted list
#     <backend> upload <local_path> <remote_path>
#
# Run with --help for usage info.

import argparse, base64, errno, functools, hashlib, io, os, re, \
       sqlite3, shlex, signal, subprocess, sys, tempfile, threading, time
sys.path.insert(0, os.path.expanduser("~/stuff/_urandom"))
import utils

HASH_ALGORITHM_PREFERENCE = [
    "sha512",
    "sha256",
    "sha1",
    "md5",
]

class UserError(Exception):
    pass

class LoopThread(threading.Thread):
    def __init__(self, interval, action, *args, **kwargs):
        self.__interval = interval
        self.__action = action
        super().__init__(*args, **kwargs)

    def run(self):
        while True:
            time.sleep(self.__interval)
            try:
                e = self.__action()
            except Exception as e:
                sys.stderr.write("warning: GnuPG loop process failed: {}\n"
                                 .format(e))
                sys.stderr.flush()

    @staticmethod
    def start_new(interval, action):
        action()
        LoopThread(interval, action, daemon=True).start()

class HashThread(threading.Thread):
    def __init__(self, hash_state, file, block_size, *args, **kwargs):
        self.__hash_state = hash_state
        self.__file = file
        self.__block_size = block_size
        super().__init__(*args, **kwargs)

    def run(self):
        for block in iter(lambda: self.__file.read(self.__block_size), b""):
            self.__hash_state.update(block)

def get_preferred_item(items, preference):
    items = set(items)
    for x in preference:
        if x in items:
            return x
    raise ValueError("no preferred item in {!r} (preferred: {!r})"
                     .format(items, preference))

def get_hasher(hash_algorithm):
    if hash_algorithm not in hashlib.algorithms_available:
        raise Exception("unsupported hash algorithm: {}"
                        .format(hash_algorithm))
    return getattr(hashlib, hash_algorithm)

def check_wait(*procs):
    errors = []
    for proc in procs:
        returncode = proc.wait()
        if returncode:
            errors.append((returncode, getattr(proc, "args", proc)))
    for returncode, args in errors:
        raise subprocess.CalledProcessError(returncode, args)

def read_blocks(in_file, size, block_size):
    while size > 0:
        block_size = min(block_size, size)
        yield in_file.read(block_size)
        size -= block_size

def get_num_chunks(size, chunk_size):
    return -(-size // chunk_size)

def get_num_digits(config, filename):
    chunk_size = config["blocks_per_chunk"] * config["block_size"]
    return len(str(get_num_chunks(os.path.getsize(filename), chunk_size) - 1))

def get_remote_chunk_indices(remote_prefix, backend):
    try:
        with open(os.devnull, "r+b") as fdevnull:
            p = subprocess.run(
                backend + ["ls", "--", os.path.dirname(remote_prefix)],
                check=True,
                stdin=fdevnull,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True)
            entries = shlex.split(p.stdout)
    except Exception:
        return []
    patt = r"^/?{0}(\d+)\.gpg".format(os.path.basename(remote_prefix))
    indices = []
    for entry in entries:
        m = re.match(patt, entry)
        if not m:
            continue
        index, = m.groups()
        indices.append(int(index))
    return indices

def get_num_chunks_from_indices(indices):
    if not indices:
        return 0
    if len(indices) != max(indices) + 1:
        raise ValueError("uh oh, remote seems to be missing chunks...")
    return len(indices)

def ping_gpg(encrypt_key, sign_key):
    with open(os.devnull, "r+b") as fdevnull:
        subprocess.run(["gpg-ping", "-e", encrypt_key, "-s",  sign_key],
                       check=True,
                       stdin=fdevnull,
                       stdout=fdevnull)

def encrypt(src_file, dst_file, encrypt_key, sign_key, **kwargs):
    return subprocess.Popen(["gpg", "-q",
                             "-e", "-r", encrypt_key,
                             "-s", "-u", sign_key,
                             "--no-armor",
                             "--compress-algo", "none"],
                            stdin=src_file,
                            stdout=dst_file,
                            **kwargs)

def decrypt(src_file, dst_file, **kwargs):
    '''Note: gpg will automatically verify the signature if it was signed and
    encrypted simultaneously.'''
    return subprocess.Popen(["gpg", "-q", "-d"],
                            stdin=src_file,
                            stdout=dst_file,
                            **kwargs)

def upload_chunk(hash_state, chunk_hasher, chunk_hash_algorithm,
                 index, local_fn, remote_prefix,
                 chunk_size, encrypt_key, sign_key, block_size, backend, **kwargs):
    remote_path = remote_prefix + "{0}.gpg".format(index)
    chunk_name = os.path.basename(remote_path)
    index = int(index)
    with open(chunk_name, "wb") as fo:
         pencrypt = encrypt(subprocess.PIPE, fo, encrypt_key, sign_key)
    with utils.ChildProcess(pencrypt):
        with open(local_fn, "rb") as fi:
            fi.seek(index * chunk_size)
            for block in read_blocks(fi, chunk_size, block_size):
                pencrypt.stdin.write(block)
            pencrypt.stdin.close()
        check_wait(pencrypt)

    # simultaneously verify data can be decrypted and compute hash of encrypted
    if hash_state is not None:
        chunk_hash_state = chunk_hasher()
        with tempfile.TemporaryFile() as ferr:
            pdecrypt = decrypt(subprocess.PIPE, subprocess.PIPE, stderr=ferr)
            try:
                with utils.ChildProcess(pdecrypt):
                    # << hash_thread begins here >>
                    hash_thread = HashThread(hash_state,
                                             pdecrypt.stdout,
                                             block_size=block_size)
                    # hash_thread owns hash_state for now
                    del hash_state
                    # prevent deadlocks due to ChildProcess trying to close it
                    # (likely because Python does file operations behind mutex)
                    pdecrypt.stdout = None
                    hash_thread.start()
                    with open(chunk_name, "rb") as fo:
                        for block in iter(lambda: fo.read(block_size), b""):
                            chunk_hash_state.update(block)
                            pdecrypt.stdin.write(block)
                    pdecrypt.stdin.close()
                    # << hash_thread ends here >>
                    hash_thread.join()
                    check_wait(pdecrypt)
            except:
                ferr.seek(0)
                sys.stderr.buffer.write(ferr.read())
                raise
        hash_arg = ["--hash",
                    chunk_hash_algorithm + ":" +
                    chunk_hash_state.hexdigest()]
    else:
        hash_arg = []

    with open(os.devnull, "r+b") as fdevnull:
        subprocess.run(backend + ["upload"] + hash_arg +
                       ["--", chunk_name, remote_path],
                       check=True,
                       stdin=fdevnull)
    utils.try_remove(chunk_name)

def download_chunk(index, remote_prefix, file, chunk_size, backend, **kwargs):
    remote_fn = remote_prefix + "{0}.gpg".format(index)
    chunk_name = os.path.basename(remote_fn)
    index = int(index)
    with open(os.devnull, "r+b") as fdevnull:
        subprocess.run(backend + ["download", "--", remote_fn, chunk_name],
                       check=True,
                       stdin=fdevnull,
                       stdout=fdevnull)
    with open(chunk_name, "rb") as fi, tempfile.TemporaryFile() as ferr:
        pdecrypt = decrypt(fi, file, stderr=ferr)
        try:
            with utils.ChildProcess(pdecrypt):
                check_wait(pdecrypt)
        except:
            ferr.seek(0)
            sys.stderr.buffer.write(ferr.read())
            raise
    utils.try_remove(chunk_name)

def rm_chunk(index, remote_prefix, backend):
    remote_fn = remote_prefix + "{0}.gpg".format(index)
    subprocess.run(backend + ["rm", "--", remote_fn], check=True)

def iterate_chunks(num_chunks):
    index_template = "{{0:0{0}}}".format(len(str(num_chunks - 1)))
    for index in range(num_chunks):
        yield index_template.format(index)

def open_output_file(fn):
    if fn == "-":
        return sys.stdout
    try:
        return open(fn, "rb+")
    except io.UnsupportedOperation:
        pass
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise
    return open(fn, "wb")

def parse_path(path):
    if path == "-":
        return path
    return os.path.abspath(path)

def md5_str(s):
    return hashlib.md5(s.encode("utf-8")).hexdigest()

def make_work_dir(**kwargs):
    if kwargs["local_fn"] == "-":       # hack
        instance_id = os.urandom(16)
    else:
        instance_id = [
            kwargs["backend"],
            kwargs["encrypt_key"],
            kwargs["sign_key"],
            kwargs["chunk_size"],
            kwargs["local_fn"],
            kwargs["remote_prefix"],
        ]
        try:
            instance_id.append(os.lstat(kwargs["local_fn"]).st_mtime_ns)
        except OSError:
            pass
    work_dir = os.path.join(
        kwargs.pop("cache_dir"),
        os.path.basename(__file__),
        kwargs["func"].__name__,
        hashlib.md5(repr(instance_id).encode("utf-8")).hexdigest(),
    )
    os.makedirs(work_dir, exist_ok=True)
    os.chdir(work_dir)
    return work_dir

def start_loop_thread(gpg_loop_interval, encrypt_key, sign_key, **kwargs):
    LoopThread.start_new(gpg_loop_interval,
                         functools.partial(ping_gpg, encrypt_key, sign_key))

# (note: __name__ of this function determines the name of the temporary dir)
def download(**kwargs):
    work_dir = make_work_dir(**kwargs)
    start_loop_thread(**kwargs)
    pos = 0
    with open_output_file(kwargs["local_fn"]) as fo:
        remote_indices = get_remote_chunk_indices(kwargs["remote_prefix"],
                                                  kwargs["backend"])
        num_chunks = get_num_chunks_from_indices(remote_indices)
        if not num_chunks:
            raise UserError("no chunks found on remote")
        for index in iterate_chunks(num_chunks):
            done_fn = os.path.join("done", index)
            if os.path.exists(done_fn):
                continue
            if int(index) != pos:
                fo.seek(int(index) * kwargs["chunk_size"])
            if kwargs["verbose"]:
                sys.stderr.write("Processing chunk #{0} ...\n".format(index))
                sys.stderr.flush()
            download_chunk(index=index, file=fo, **kwargs)
            fo.flush()
            pos += 1
            os.makedirs(os.path.dirname(done_fn), exist_ok=True)
            with open(done_fn, "w") as f:
                pass
    for index in iterate_chunks(num_chunks):
        done_fn = os.path.join("done", index)
        os.remove(done_fn)
    os.rmdir("done")
    os.chdir("/")
    os.rmdir(work_dir)

def fast_forward_hash_state(db, hash_state, hash_state_index, index,
                            local_fn, chunk_size, block_size):
    if hash_state is None or index == 0:
        return hash_state_index
    assert hash_state_index <= index
    if hash_state_index < index:
        # fast-forward the hash_state
        with open(local_fn, "rb") as fi:
            fi.seek(hash_state_index * chunk_size)
            size = chunk_size * (index - hash_state_index)
            for block in read_blocks(fi, size, block_size):
                hash_state.update(block)
        hash_state_index = index
        old_hash_digest = tuple(db.execute('SELECT "hash" FROM "chunks" '
                                           'WHERE "index" = ?',
                                           [index - 1]))[0][0]
        if hash_state.hexdigest() != old_hash_digest:
            raise Exception("can't restore original hash state: "
                            "hash mismatch (did the local file change?):\n"
                            "{!r}\n{!r}".format(hash_state.hexdigest(),
                                                old_hash_digest))
    return hash_state_index

# (note: __name__ of this function determines the name of the temporary dir)
def upload(**kwargs):
    work_dir = make_work_dir(**kwargs)
    start_loop_thread(**kwargs)
    db = sqlite3.connect("job") # FIXME: use contextlib
    db.executescript("""
    CREATE TABLE IF NOT EXISTS "chunks"
    ( "index" INTEGER PRIMARY KEY
    , "hash" TEXT
    ) ;
    CREATE TABLE IF NOT EXISTS "metadata"
    ( "key" TEXT UNIQUE
    , "value" TEXT
    ) ;
    """)

    # hash_state: for hashing re-decrypted data
    # chunk_hasher: for hashing encrypted chunks
    if kwargs["hash"]:
        hash_algorithm, hash_prefix = kwargs["hash"].split(":", 1)
        hash_state = get_hasher(hash_algorithm)()
        backend_hash_algorithms = subprocess.run(
            kwargs["backend"] + ["hash-algorithms"],
            stdout=subprocess.PIPE,
            check=True,
            universal_newlines=True).stdout.split()
        chunk_hash_algorithm = get_preferred_item(
            backend_hash_algorithms, HASH_ALGORITHM_PREFERENCE)
        chunk_hasher = get_hasher(chunk_hash_algorithm)
        for old_hash_algorithm, in db.execute('SELECT "value" FROM "metadata" '
                                              ' WHERE "key" = ?',
                                              ["hash_algorithm"]):
            if hash_algorithm != old_hash_algorithm:
                raise Exception("can't resume with different hash algorithm")
        with db:
            db.execute("BEGIN")
            db.execute('INSERT OR REPLACE INTO "metadata" ("key", "value") '
                       'VALUES (?, ?)',
                       ["hash_algorithm", hash_algorithm])
    else:
        hash_state = None
        chunk_hasher = None

    local_fn = kwargs["local_fn"]
    size = os.path.getsize(local_fn)
    chunk_size = kwargs["chunk_size"]
    block_size = kwargs["block_size"]
    num_chunks = get_num_chunks(size, chunk_size)
    remote_indices = get_remote_chunk_indices(kwargs["remote_prefix"],
                                              kwargs["backend"])
    hash_state_index = 0
    for index in iterate_chunks(num_chunks):
        if tuple(db.execute('SELECT 1 FROM "chunks" WHERE "index" = ?',
                            [int(index)])):
            if int(index) not in remote_indices:
                raise ValueError("Chunk #{0} is missing from remote? "
                                 "To retry, clear your cache dir."
                                 .format(index))
            continue
        if kwargs["verbose"]:
            sys.stderr.write("Processing chunk #{0} ...\n".format(index))
            sys.stderr.flush()
        hash_state_index = fast_forward_hash_state(db, hash_state,
                                                   hash_state_index,
                                                   int(index), local_fn,
                                                   chunk_size, block_size)
        upload_chunk(hash_state=hash_state,
                     chunk_hasher=chunk_hasher,
                     chunk_hash_algorithm=chunk_hash_algorithm,
                     index=index,
                     **kwargs)
        with db:
            db.execute("BEGIN")
            if hash_state is None:
                db.execute('INSERT INTO "chunks" ("index") VALUES (?)',
                           [int(index)])
            else:
                hash_state_index += 1
                # persist the current hash for future resumptions
                db.execute('INSERT INTO "chunks" ("index", "hash") '
                           'VALUES (?, ?)',
                           [int(index), hash_state.hexdigest()])

    hash_state_index = fast_forward_hash_state(db, hash_state,
                                               hash_state_index,
                                               num_chunks, local_fn,
                                               chunk_size, block_size)
    if hash_state is not None:
        if not hash_state.hexdigest().startswith(hash_prefix):
            raise Exception("hash mismatch:\n"
                            "uploaded: {!r}\n"
                            "provided: {!r}"
                            .format(hash_state.hexdigest(), hash_prefix))

    db.close()
    os.remove("job")
    os.chdir("/")
    os.rmdir(work_dir)

def exists(**kwargs):
    remote_indices = get_remote_chunk_indices(kwargs["remote_prefix"],
                                              kwargs["backend"])
    num_chunks = get_num_chunks_from_indices(remote_indices)
    if not num_chunks:
        raise UserError("no chunks found on remote")
    sys.stdout.write("found {} chunks\n".format(num_chunks))

def rm(**kwargs):
    remote_indices = get_remote_chunk_indices(kwargs["remote_prefix"],
                                              kwargs["backend"])
    num_chunks = get_num_chunks_from_indices(remote_indices)
    if not num_chunks:
        raise UserError("no chunks found on remote")
    for index in iterate_chunks(num_chunks):
        rm_chunk(index, kwargs["remote_prefix"], kwargs["backend"])

def main():
    p = argparse.ArgumentParser(description=(
        "Utility for transferring large files to cloud storage.  "
        "It is recommended to set the cache directory (via the `-C` flag) "
        "to an in-memory file system for maximum performance.  "
        "Make sure the directory is secure and has enough room "
        "for at least three full chunks."))
    p.add_argument("-C", dest="cache_dir",
                   required=True,
                   help="the cache directory used for all operations")
    p.add_argument("-t", "--store", dest="backend", required=True,
                   type=shlex.split, help="storage backend command")
    p.add_argument("-k", "--key", dest="encrypt_key", required=True,
                   help="GPG key ID used for encryption")
    p.add_argument("-s", "--sign-key", required=True,
                   help="GPG key ID used for signing")
    p.add_argument("--directory", default=False, action="store_true",
                   help="force remote prefix to be a directory prefix")
    p.add_argument("--chunk-size", type=int, default=(1 << 30),
                   help="size of each chunk in bytes (default: 1GiB)")
    p.add_argument("--block-size", type=int, default=(1 << 20),
                   help=("size of each read block in bytes (default: 1MiB); "
                         "only affects the efficiency of the upload process"))
    p.add_argument("--gpg-loop-interval", type=float, default=120,
                   help="how often to ping GPG to keep the keys in memory")
    p.add_argument("--verbose", action="store_true")
    sp = p.add_subparsers()

    spp = sp.add_parser("download")
    spp.set_defaults(func=download)
    spp.add_argument("remote_prefix",
                     help="prefix of the remote files")
    spp.add_argument("local_fn", metavar="local_file", type=parse_path,
                     help="filename of the local file (use `-` for stdout)")

    spp = sp.add_parser("upload")
    spp.set_defaults(func=upload)
    spp.add_argument("--hash", help="hash used for verification "
                     "(format: <algorithm>:<hex-digest-prefix>)")
    spp.add_argument("local_fn", metavar="local_file", type=os.path.abspath,
                     help="filename of the local file")
    spp.add_argument("remote_prefix",
                     help="prefix of the remote files")

    spp = sp.add_parser("exists")
    spp.set_defaults(func=exists)
    spp.add_argument("remote_prefix",
                     help="prefix of the remote files")

    spp = sp.add_parser("rm")
    spp.set_defaults(func=rm)
    spp.add_argument("remote_prefix",
                     help="prefix of the remote files")

    spp = sp.add_parser("ping")
    spp.set_defaults(func=start_loop_thread)

    kwargs = vars(p.parse_args())
    func = kwargs.get("func", None)
    if not func:
        p.print_usage()
        sys.exit(2)
    if kwargs["directory"] and "remote_prefix" in kwargs:
        kwargs["remote_prefix"] = os.path.join(kwargs["remote_prefix"], "")
    try:
        func(**kwargs)
    except UserError as e:
        sys.stderr.write("cloud-transfer: {}\n".format(e))
        sys.stderr.flush()
        sys.exit(1)

signal.signal(signal.SIGINT, signal.SIG_DFL)
with utils.SignalsToExceptions():
    main()
